{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vE4pWW-pzZSN","executionInfo":{"status":"ok","timestamp":1669533473707,"user_tz":-480,"elapsed":310,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"12b3e923-ad5d-453e-825a-cb0bbe42c145"},"outputs":[{"output_type":"stream","name":"stdout","text":["vocabuary \n"," ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n","Sentence to vector \n"," [[0 1 1 1 0 0 1 0 1]\n"," [0 2 0 1 0 1 1 0 1]\n"," [1 0 0 1 1 0 1 1 1]\n"," [0 1 1 1 0 0 1 0 1]]\n","[1.         0.79056942 0.54772256 1.        ]\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","import numpy as np\n","corpus = [\n","    'This is the first document.',\n","    'This document is the second document.',\n","    'And this is the third one.',\n","    'Is this the first document?',\n","]\n","# corpus =['Lewis Yang', 'ada Yang']\n","vectorizer = CountVectorizer() #準備在記憶體之中\n","X = vectorizer.fit_transform(corpus) # 真正餵入資料轉換上述LIST成為   代碼表\n","# X.toarray(), np.sum(X.toarray(), axis=1)\n","print('vocabuary', '\\n', vectorizer.get_feature_names_out()) # 單字 Tokenizer\n","print('Sentence to vector', '\\n', X.toarray() )   # 代碼表轉換成矩陣  代碼表\n","\n","from sklearn.metrics import pairwise\n","# print(pairwise.cosine_similarity( X.toarray()[0].reshape(-1,1),  \n","#                                   X.toarray()[1].reshape(-1,1)))\n","print(pairwise.cosine_similarity(X[0], X).ravel())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rrIWYuEAzZSQ","executionInfo":{"status":"ok","timestamp":1669531383297,"user_tz":-480,"elapsed":11,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"7c2ac57a-13ff-4b35-ccdb-b8bcf825260f"},"outputs":[{"output_type":"stream","name":"stdout","text":["['document' 'first' 'is' 'the' 'this']\n","['document' 'is' 'second' 'the' 'this']\n","['and' 'is' 'one' 'the' 'third' 'this']\n","['document' 'first' 'is' 'the' 'this']\n"]}],"source":["for d in range(len(corpus)):\n","    words = np.where( X.toarray()[d, :] ==0 , False, True) \n","    print(np.array(vectorizer.get_feature_names_out())[words])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDdwOBzjzZSR","executionInfo":{"status":"ok","timestamp":1669531436125,"user_tz":-480,"elapsed":17,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"8bcf3a17-ec07-4db9-a0d3-91b044c2062d"},"outputs":[{"output_type":"stream","name":"stdout","text":["vocabuary \n"," ['ada' 'and' 'bobo' 'julia' 'kuo' 'lewis' 'li' 'yang']\n","Sentence to vector \n"," [[0 0 0 0 0 1 0 1]\n"," [1 0 0 0 1 0 0 0]\n"," [0 1 1 1 0 0 1 1]\n"," [1 0 0 0 0 0 0 1]]\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","# corpus = [\n","#     'This is the first document.',\n","#     'This document is the second document.',\n","#     'And this is the third one.',\n","#     'Is this the first document?',\n","# ]\n","corpus =['Lewis Yang', 'Ada-Kuo', \"Julia Li and BoBo Yang\", 'Ada Yang']\n","\n","vectorizer = CountVectorizer() #準備在記憶體之中\n","X = vectorizer.fit_transform(corpus) # 真正餵入資料轉換上述LIST成為   代碼表\n","\n","print('vocabuary', '\\n', vectorizer.get_feature_names_out()) # 單字 Tokenizer\n","print('Sentence to vector', '\\n', X.toarray() )   # 代碼表轉換成矩陣  代碼表"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Di5x2-pwzZSS","executionInfo":{"status":"ok","timestamp":1669531484281,"user_tz":-480,"elapsed":390,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"ed804e15-2d32-47e5-de3f-cfc905ebdbd4"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0 0 0 0 1 0 1]]\n"]},{"output_type":"execute_result","data":{"text/plain":["[array(['lewis', 'yang'], dtype='<U5')]"]},"metadata":{},"execution_count":5}],"source":["import numpy as np\n","#print(dir(vectorizer))\n","text= ['I am lewis yang'] # 原文\n","data = vectorizer.transform(text).toarray() # 經過轉換\n","print(data) # 變成代碼表\n","vectorizer.inverse_transform(data) # 反代碼表成為文字"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dqmht6sbzZST"},"outputs":[],"source":["# <4x9 sparse matrix of type '<class 'numpy.int64'>'\n","# \twith 21 stored elements in Compressed Sparse Row format>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l5AQB1oVzZST","executionInfo":{"status":"ok","timestamp":1669531503056,"user_tz":-480,"elapsed":376,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"9223e42c-b9ae-40a1-c20b-b36523b2d38e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 0, 0, 0, 0, 1, 0, 1],\n","       [1, 0, 0, 0, 1, 0, 0, 0],\n","       [0, 1, 1, 1, 0, 0, 1, 1],\n","       [1, 0, 0, 0, 0, 0, 0, 1]])"]},"metadata":{},"execution_count":6}],"source":["X.toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nDSHRjYZzZSU","executionInfo":{"status":"ok","timestamp":1669531530848,"user_tz":-480,"elapsed":9,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"3a02968d-f179-4959-82b8-e6343e0780fe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array(['ada', 'and', 'bobo', 'julia', 'kuo', 'lewis', 'li', 'yang'],\n","       dtype=object),\n"," array([ True, False, False, False, False, False, False,  True]))"]},"metadata":{},"execution_count":7}],"source":["np.array(vectorizer.get_feature_names_out()), X.toarray()[3].astype('bool')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4vUEnrBgzZSV","executionInfo":{"status":"ok","timestamp":1669531651573,"user_tz":-480,"elapsed":397,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"2244e70a-ba3e-48a3-f77f-452bd6623795"},"outputs":[{"output_type":"stream","name":"stdout","text":["['lewis' 'yang']\n","['ada' 'kuo']\n","['and' 'bobo' 'julia' 'li' 'yang']\n","['ada' 'yang']\n"]}],"source":["import numpy as np\n","# 根據 X.toarray() 對應出字典單字\n","words = np.array(vectorizer.get_feature_names_out())\n","for i in range(len(corpus)):\n","    print( words[(X.toarray()[i]).astype('bool')] )\n","# corpus =['Lewis Yang', 'Ada-Kuo', \"Julia Li and BoBo Yang\", 'Ada Yang']    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yPgOdsvozZSV","executionInfo":{"status":"ok","timestamp":1669531669703,"user_tz":-480,"elapsed":459,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"a73d977e-11fe-4fd2-912f-acfd71e34848"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['ada', 'and', 'bobo', 'julia', 'kuo', 'lewis', 'li', 'yang'],\n","      dtype=object)"]},"metadata":{},"execution_count":9}],"source":["import numpy as np\n","words = np.array(vectorizer.get_feature_names_out())\n","words"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J-ug9O9rzZSW","executionInfo":{"status":"ok","timestamp":1669531685506,"user_tz":-480,"elapsed":385,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"490cd8df-697d-4c25-af88-5358dbca3c86"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['lewis', 'yang'], dtype=object)"]},"metadata":{},"execution_count":10}],"source":["words[(X.toarray())[0].astype('bool')]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XPdfp0VMzZSX","executionInfo":{"status":"ok","timestamp":1669531696445,"user_tz":-480,"elapsed":366,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"d98e16e8-e2a9-4cbc-f50a-7946d3ef5556"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['Lewis', 'Yang'],\n"," ['Ada-Kuo'],\n"," ['Julia', 'Li', 'and', 'BoBo', 'Yang'],\n"," ['Ada', 'Yang']]"]},"metadata":{},"execution_count":11}],"source":["[i.split() for i in corpus]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TLaXYP2IzZSX","executionInfo":{"status":"ok","timestamp":1669531715331,"user_tz":-480,"elapsed":406,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"e16820e6-2276-4e09-e847-4dba586bf448"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n","       'this'], dtype='<U8')"]},"metadata":{},"execution_count":12}],"source":["#2-layer list comprehension 手動方式\n","corpus = [\n","    'This is the first document',\n","    'This document is the second document',\n","    'And this is the third one',\n","    'Is this the first document',\n","]\n","mydict = []\n","for i in corpus:\n","    for j in i.lower().split():\n","        mydict.append(j)\n","np.unique(mydict)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"Dhwtjw1FzZSY","executionInfo":{"status":"ok","timestamp":1669531756585,"user_tz":-480,"elapsed":7,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"8be73c6a-62a1-45ed-cc6e-ad6d21a64ff1"},"outputs":[{"output_type":"stream","name":"stdout","text":["vocabuary \n"," ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n","Sentence to vector \n"," [[0 1 1 1 0 0 1 0 1]\n"," [0 2 0 1 0 1 1 0 1]\n"," [1 0 0 1 1 0 1 1 1]\n"," [0 1 1 1 0 0 1 0 1]]\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","corpus = [\n","    'This is the first document',\n","    'This document is the second document',\n","    'And this is the third one',\n","    'Is this the first document',\n","]\n","vectorizer = CountVectorizer() #準備在記憶體之中\n","X = vectorizer.fit_transform(corpus) # 真正餵入資料轉換上述LIST成為   代碼表\n","\n","print('vocabuary', '\\n', vectorizer.get_feature_names_out()) # 單字 Tokenizer\n","print('Sentence to vector', '\\n', X.toarray() )   # 代碼表轉換成矩陣  代碼表"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BglxlzdzzZSZ","executionInfo":{"status":"ok","timestamp":1669531774287,"user_tz":-480,"elapsed":370,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"c29ffc17-669d-4859-e1c2-6450994fb05f"},"outputs":[{"output_type":"stream","name":"stdout","text":["vocabuary \n"," ['document' 'second']\n","Sentence to vector \n"," [[1 0]\n"," [2 1]\n"," [0 0]\n"," [1 0]]\n"]}],"source":["# 加入 sklearn 的 stopwords\n","from sklearn.feature_extraction.text import CountVectorizer\n","corpus = [\n","    'This is the first document',\n","    'This document is the second document',\n","    'And this is the third one',\n","    'Is this the first document',\n","]\n","vectorizer = CountVectorizer(stop_words='english') #準備在記憶體之中\n","X = vectorizer.fit_transform(corpus) # 真正餵入資料轉換上述LIST成為   代碼表\n","\n","print('vocabuary', '\\n', vectorizer.get_feature_names_out()) # 單字 Tokenizer\n","print('Sentence to vector', '\\n', X.toarray() )   # 代碼表轉換成矩陣  代碼表"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KXd-BGeczZSZ","executionInfo":{"status":"ok","timestamp":1669531820504,"user_tz":-480,"elapsed":349,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"9b3ddb17-3562-4403-9a04-8af0b26a7dc6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n","       'this'], dtype='<U8')"]},"metadata":{},"execution_count":16}],"source":["# 根據 自己切字 找出唯一\n","np.unique([j for i in corpus for j in i.lower().split()])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H1GRty-izZSa","executionInfo":{"status":"ok","timestamp":1669531826005,"user_tz":-480,"elapsed":350,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"cc56015d-a73b-44e0-a66b-917f869a24e7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n","       'this'], dtype='<U8')"]},"metadata":{},"execution_count":17}],"source":["# 自己切字\n","# 2-layer list comprehension 手動方式\n","corpus = [\n","    'This is the first document',\n","    'This document is the second document',\n","    'And this is the third one',\n","    'Is this the first document',\n","]\n","\n","np.unique([j.lower() for i in corpus for j in i.split() ])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hBVKyp_mzZSb","executionInfo":{"status":"ok","timestamp":1669531829163,"user_tz":-480,"elapsed":350,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"6625c56c-8fc1-4f2e-8cf2-5cb8ebe93245"},"outputs":[{"output_type":"stream","name":"stdout","text":["This\n","is\n","the\n","first\n","document\n","This\n","document\n","is\n","the\n","second\n","document\n","And\n","this\n","is\n","the\n","third\n","one\n","Is\n","this\n","the\n","first\n","document\n"]}],"source":["for i in corpus:\n","    for j in i.split():\n","        print(j)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d9zRCtpRzZSd","executionInfo":{"status":"ok","timestamp":1669531998043,"user_tz":-480,"elapsed":6,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"3e2ad388-b220-4089-adcb-982541dad24b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":23}],"source":["# scikit-learn stopwords\n","from sklearn.feature_extraction import _stop_words as stop_words\n","'is' in  (stop_words.ENGLISH_STOP_WORDS)"]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MkmYlxEu3O_V","executionInfo":{"status":"ok","timestamp":1669532031937,"user_tz":-480,"elapsed":5,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"f97b02f5-5e0e-4c12-e493-54e7c0cef17f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G9ERn83kzZSe","executionInfo":{"status":"ok","timestamp":1669532035814,"user_tz":-480,"elapsed":532,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"58dd2477-c741-4ec8-87e6-d90302da0ed3"},"outputs":[{"output_type":"stream","name":"stdout","text":["and  in english_stop_words\n","is  in english_stop_words\n","the  in english_stop_words\n","this  in english_stop_words\n"]}],"source":["# nltk stopwords\n","# !pip install nltk\n","\n","from nltk.corpus import stopwords\n","english_stops = set(stopwords.words('english'))\n","\n","allwords =  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n","for a in allwords :\n","    if a in english_stops :\n","        print(a , ' in english_stop_words')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}