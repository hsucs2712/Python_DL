{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0BpQSufKwyBN","outputId":"6ec7454e-bb70-4bb6-893a-72dc659a2f54"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING: You are using pip version 20.1.1; however, version 20.3.1 is available.\n","You should consider upgrading via the 'c:\\python37\\python.exe -m pip install --upgrade pip' command.\n"]},{"name":"stdout","output_type":"stream","text":["Collecting jieba\n","  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n","Building wheels for collected packages: jieba\n","  Building wheel for jieba (setup.py): started\n","  Building wheel for jieba (setup.py): finished with status 'done'\n","  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314482 sha256=39cf2cdaebcd128b1d6c9a9ea4752a12fdc2f6be13d13a7a7927e9b2e361c1fe\n","  Stored in directory: c:\\users\\lewis_yang\\appdata\\local\\pip\\cache\\wheels\\24\\aa\\17\\5bc7c72e9a37990a9620cc3aad0acad1564dcff6dbc2359de3\n","Successfully built jieba\n","Installing collected packages: jieba\n","Successfully installed jieba-0.42.1\n"]}],"source":["!pip install jieba"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"E3XrtSovwyBU","executionInfo":{"status":"ok","timestamp":1669530370087,"user_tz":-480,"elapsed":986,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"a1fe99e5-5af6-4ff1-afc3-3cf44616bf50"},"outputs":[{"output_type":"stream","name":"stderr","text":["Building prefix dict from the default dictionary ...\n","DEBUG:jieba:Building prefix dict from the default dictionary ...\n","Dumping model to file cache /tmp/jieba.cache\n","DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n","Loading model cost 1.770 seconds.\n","DEBUG:jieba:Loading model cost 1.770 seconds.\n","Prefix dict has been built successfully.\n","DEBUG:jieba:Prefix dict has been built successfully.\n"]},{"output_type":"execute_result","data":{"text/plain":["['這是', '第一篇', '文章', '.']"]},"metadata":{},"execution_count":1}],"source":["import jieba\n","list(jieba.cut(sentence = '這是第一篇文章.'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x2D5XPTtwyBX","executionInfo":{"status":"ok","timestamp":1669530380288,"user_tz":-480,"elapsed":2617,"user":{"displayName":"Lewis Yang","userId":"11249979140389061982"}},"outputId":"18dc42c0-d8ba-49e0-f900-d03c9b80d4d5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['並且', '嗎', '文章', '是', '第一篇', '第三篇', '第二篇', '這', '這是'], dtype='<U3')"]},"metadata":{},"execution_count":2}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","import numpy as np\n","corpus = [\n","    '這是第一篇文章',\n","    '這文章是第二篇',\n","    '並且這是第三篇',\n","    '這是第一篇文章嗎',\n","]\n","words = []\n","for c in corpus :\n","    words.extend( list(jieba.cut(sentence = c )) )\n","np.unique( words )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKqYFJdcwyBa","outputId":"023a263b-966e-4bfc-e86a-0864b0ce6a62"},"outputs":[{"data":{"text/plain":["['這是', '第一篇', '文章']"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# 手動轉換成 \n","# Sentence to vector \n","#  [[0 1 1 1 0 0 1 0 1]\n","#  [0 2 0 1 0 1 1 0 1]\n","#  [1 0 0 1 1 0 1 1 1]\n","#  [0 1 1 1 0 0 1 0 1]]\n","\n","list(jieba.cut(sentence = '這是第一篇文章' ))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fV4CH3nOwyBb","outputId":"4dd9c1c2-716e-4e81-b4c1-4399f878b599"},"outputs":[{"data":{"text/plain":["array([[0, 0, 1, 0, 1, 0, 0, 0, 1],\n","       [0, 0, 1, 1, 0, 0, 1, 1, 0],\n","       [1, 0, 0, 0, 0, 1, 0, 0, 1],\n","       [0, 1, 1, 0, 1, 0, 0, 0, 1]])"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["# 思考\n","# ['這是', '第一篇', '文章'] \n","# ['並且', '嗎', '文章', '是', '第一篇', '第三篇', '第二篇', '這', '這是']\n","# [0    ,    0,      1,    0,        1,       0,        0,   0,      1]\n","zh_all = np.unique( words )\n","zh_cv = np.zeros([4,9]).astype('int')\n","###############################################\n","#字頻分析表\n","##############################################\n","# 從文章中取出 每一 kk=序號， vv=每一條文章內容\n","for kk,vv in enumerate(corpus):\n","    # 針對 vv 每一個文章內容去切字\n","    for v in list(jieba.cut(sentence = vv )):\n","        # 針對 每一個單字 進行 轉換 1 \n","        pos = np.where(zh_all == v)\n","        zh_cv[kk, pos] = 1\n","zh_cv "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_KiAoxujwyBd","outputId":"da95b108-97e2-436d-a9bd-207e2cfe2686"},"outputs":[{"data":{"text/plain":["array([[0, 0, 1, 0, 1, 0, 0, 0, 1],\n","       [0, 0, 1, 1, 0, 0, 1, 1, 0],\n","       [1, 0, 0, 0, 0, 1, 0, 0, 1],\n","       [0, 1, 1, 0, 1, 0, 0, 0, 1]])"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["zh_cv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsriR6KgwyBe","outputId":"24e9d438-d1ad-4d63-ed53-15acc77398d0"},"outputs":[{"data":{"text/plain":["array([1.        , 0.28867513, 0.33333333, 0.8660254 ])"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","cosine_similarity(zh_cv[0].reshape(1,-1), zh_cv).ravel()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"elF-7BsDwyBi"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9gC2eaEwyBj"},"outputs":[],"source":["# corpus = [\n","#     '這是第一篇文章',\n","#     '這文章是第二篇',\n","#     '並且這是第三篇',\n","#     '這是第一篇文章嗎',\n","# ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10HNQz8TwyBl","outputId":"ed4335ee-d162-4bf0-eb34-703bacaa4452"},"outputs":[{"name":"stdout","output_type":"stream","text":["['這是 第一篇 文章', '這 文章 是 第二篇', '並且 這是 第三篇', '這是 第一篇 文章 嗎']\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","import numpy as np\n","corpus = [\n","    '這是第一篇文章',\n","    '這文章是第二篇',\n","    '並且這是第三篇',\n","    '這是第一篇文章嗎',\n","]\n","words = []\n","for c in corpus :\n","    words.append( ' '.join(list(jieba.cut(sentence = c ))) )\n","\n","print(words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9jddUdRtwyBo","outputId":"0539d4bf-3298-4d17-a8b9-a5c2872590f6"},"outputs":[{"data":{"text/plain":["array([[0.        , 0.        , 0.53256952, 0.        , 0.65782931,\n","        0.        , 0.        , 0.        , 0.53256952],\n","       [0.        , 0.        , 0.34578314, 0.5417361 , 0.        ,\n","        0.        , 0.5417361 , 0.5417361 , 0.        ],\n","       [0.64450299, 0.        , 0.        , 0.        , 0.        ,\n","        0.64450299, 0.        , 0.        , 0.41137791],\n","       [0.        , 0.64065543, 0.40892206, 0.        , 0.5051001 ,\n","        0.        , 0.        , 0.        , 0.40892206]])"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","transformer = TfidfTransformer()\n","tfidf = transformer.fit_transform(zh_cv)\n","tfidf.toarray().shape # 四篇文章 所有的單字 \n","tfidf.toarray() # 該部分類似 CountVectorizer() 顯示單字出現的頻率，加上權重"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-dDnK21wyBr","outputId":"a8451322-a6d5-4fdb-e7f8-8a5a7fdee288"},"outputs":[{"data":{"text/plain":["array([0.76782851, 0.14139835, 0.1682215 , 1.        ])"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["base = 3\n","similarity_subject = np.array(cosine_similarity(tfidf[base], tfidf)).ravel()\n","similarity_subject"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}