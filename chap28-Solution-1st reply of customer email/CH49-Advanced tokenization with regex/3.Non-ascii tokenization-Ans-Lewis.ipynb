{"cells":[{"cell_type":"markdown","metadata":{"id":"GSvLxJEr5yQ9"},"source":["# Á∑¥Áøí\n","## Non-ascii tokenization\n","In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n","\n","Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji and the German characters!\n","\n","The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.\n","\n","Unicode ranges for emoji are:\n","\n","('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF')."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UL2BtRYV5yRr"},"outputs":[],"source":["# Ê∫ñÂÇô‰∫ãÈ†Ö\n","# Tokenize all the words in german_text using word_tokenize(), and print the result.\n","# Tokenize only the capital words in german_text.\n","# First, write a pattern called capital_words to match only capital words. Make sure to check for the German √ú! To use this character in the exercise, copy and paste it from these instructions.\n","# Then, tokenize it using regexp_tokenize().\n","# Tokenize only the emoji in german_text. The pattern using the unicode ranges for emoji given in the assignment text has been written for you. Your job is to use regexp_tokenize() to tokenize the emoji.\n","german_text = 'Wann gehen wir Pizza essen? üçï Und f√§hrst du mit √úber? üöï'\n","\n","from nltk.tokenize import regexp_tokenize, word_tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XS8NlLFL5yRx","outputId":"0f0c1e4b-68e2-49cd-87c3-ba24134592d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'üçï', 'Und', 'f√§hrst', 'du', 'mit', '√úber', '?', 'üöï']\n","['Wann', 'Pizza', 'Und']\n","['üçï', 'üöï']\n"]}],"source":["# Tokenize and print all words in german_text\n","all_words = word_tokenize(german_text)\n","print(all_words)\n","\n","# Tokenize and print only capital words\n","capital_words = r\"[A-Z]\\w+\"\n","print(regexp_tokenize(german_text, capital_words))\n","\n","# Tokenize and print only emoji\n","emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n","print(regexp_tokenize(german_text, emoji))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IPpG0Tb05yR5"},"outputs":[],"source":["# È†êÊúüÁµêÊûú\n","# ['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'üçï', 'Und', 'f√§hrst', 'du', 'mit', '√úber', '?', 'üöï']\n","# ['Wann', 'Pizza', 'Und', '√úber']\n","# ['üçï', 'üöï']"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}